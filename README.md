# diffGrad
diffGrad: An Optimization Method for Convolutional Neural Networks

<b>Abstract—</b>Stochastic Gradient Decent (SGD) is one of the core techniques behind the success of deep neural networks. The gradient provides information on the direction in which the function has the steepest rate of change. The main problem with SGD is to increase by equal steps for all parameters irrespective of gradient behavior. Thus, one of the important aspects of high-dimensional optimization problem is to have the adaptive step size for each parameter. Recently, several attempts have been made to improve the gradient descent methods such as AdaGrad, AdaDelta, RMSProp and Adam. These methods rely on the square roots of exponential moving averages of squared past gradients. Thus, these methods can not take the advantage of local change in gradients. In this paper, a novel optimizer is proposed based on the difference between the present and the immediate past gradient (i.e., diffGrad). In the proposed diffGrad optimization technique, the step size is adjusted for each parameter in such a way that it should have a larger step size for faster gradient changing parameters and lower step size for lower gradient changing parameters. The convergence analysis is done using regret bound approach of online learning framework. Rigorous analysis is made in this paper over three synthetic complex non-convex functions. The image categorization experiments are also conducted over CIFAR10 and CIFAR100 datasets to observe the performance of diffGrad with respect to the state-of-the-art optimizers such as SGDM, AdaGrad, AdaDelta, RMSProp, AMSGrad, and Adam. The residual unit (ResNet) based convolutional neural networks (CNN) architecture is used in the experiments. Our experiments show that diffGrad outperforms the other optimizers. The effect of different activation function is also analyzed with ResNet50 for the proposed optimizer.




Copyright (©2019): Shiv Ram Dubey, Indian Institute of Information Technology, Sri City, Chittoor, A.P., India
